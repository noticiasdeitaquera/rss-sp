import time
import hashlib
import json
import re
import requests
from feedgen.feed import FeedGenerator
from flask import Flask, Response
from datetime import datetime, timezone, timedelta
from urllib.parse import urljoin, urlparse

app = Flask(__name__)

# üîß Configura√ß√µes gerais
BASE_URL = "https://prefeitura.sp.gov.br"
NEWS_PAGE = f"{BASE_URL}/noticias"

# üîß Estruturas conhecidas e gen√©ricas (multi-fonte)
# STRUCTURE_IDS ser√° analisado dinamicamente na p√°gina; esta lista √© fallback
STRUCTURE_IDS_FALLBACK = [79914]  # adicione IDs conhecidos aqui se desejar

GENERIC_SOURCES = [
    f"{BASE_URL}/o/headless-delivery/v1.0/sites/34276/structured-contents?pageSize=100&sort=datePublished:desc"
]

# üîß Imagem padr√£o
DEFAULT_IMAGE = "https://www.noticiasdeitaquera.com.br/imagens/logoprefsp.png"

# üîß Filtros configur√°veis
INCLUDE_KEYWORDS = []  # se vazio ‚Üí todas entram
EXCLUDE_KEYWORDS = []  # se vazio ‚Üí nenhuma √© exclu√≠da

# üîß Requisitos m√≠nimos
MIN_ITEMS = 10  # garante no m√≠nimo 10 not√≠cias publicadas

# üîß Sess√£o HTTP
SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "Mozilla/5.0 (RSS Generator; +https://rss-sp.onrender.com)"})
TIMEOUT = 10

# üîß Cache persistente e em mem√≥ria
CACHE = {"feed": None, "ts": 0}
CACHE_TTL = 600  # 10 minutos
DISK_CACHE_FILE = "/tmp/rss_sp_cache.xml"  # caminho persistente no Render


# Utilidades ---------------------------------------------------------------

def safe_title(item):
    """Retorna t√≠tulo seguro (string ou dict)."""
    raw_title = item.get("title")
    if isinstance(raw_title, dict):
        return raw_title.get("pt_BR") or "Sem t√≠tulo"
    if isinstance(raw_title, str):
        return raw_title
    return "Sem t√≠tulo"


def safe_date(pub_date):
    """Retorna datetime seguro (ISO ou agora)."""
    dt = datetime.now(timezone.utc)
    if pub_date:
        try:
            # Liferay usa Z UTC; normaliza para +00:00
            dt = datetime.fromisoformat(pub_date.replace("Z", "+00:00"))
        except Exception:
            pass
    return dt


def normalize_url(url):
    """Torna uma URL absoluta usando BASE_URL."""
    if not url:
        return None
    # se j√° for absoluta, retorna
    if bool(urlparse(url).netloc):
        return url
    # se come√ßar com '/', junta com dom√≠nio
    return urljoin(BASE_URL, url)


def normalize_image_url(url):
    """Normaliza URL de imagem para formato absoluto completo."""
    normalized = normalize_url(url)
    # Alguns endpoints retornam 'contentUrl' relativo dentro de `image`
    return normalized or DEFAULT_IMAGE


def read_disk_cache():
    """L√™ √∫ltimo feed persistido em disco."""
    try:
        with open(DISK_CACHE_FILE, "rb") as f:
            return f.read()
    except Exception:
        return None


def write_disk_cache(data: bytes):
    """Escreve feed persistido em disco."""
    try:
        with open(DISK_CACHE_FILE, "wb") as f:
            f.write(data)
    except Exception:
        pass


# Descoberta de fontes (estilo rss.app) -----------------------------------

def discover_structure_ids_from_page():
    """
    Analisa a p√°gina de not√≠cias para encontrar IDs de estruturas utilizados.
    Procura padr√µes de 'content-structures/{id}/structured-contents' em scripts/HTML.
    """
    try:
        resp = SESSION.get(NEWS_PAGE, timeout=TIMEOUT)
        if resp.status_code != 200 or not resp.text:
            return STRUCTURE_IDS_FALLBACK.copy()

        html = resp.text

        # Encontra poss√≠veis chamadas a content-structures/{ID}
        ids = set()

        # Regex para capturar IDs num√©ricos ap√≥s 'content-structures/'
        for match in re.findall(r"content-structures/(\d+)/structured-contents", html):
            try:
                ids.add(int(match))
            except Exception:
                continue

        # Busca tamb√©m em URLs codificados (com %2F etc.)
        for match in re.findall(r"content-structures%2F(\d+)%2Fstructured-contents", html):
            try:
                ids.add(int(match))
            except Exception:
                continue

        # Se nada encontrado, retorna fallback
        found = list(ids)
        if not found:
            return STRUCTURE_IDS_FALLBACK.copy()

        return found
    except Exception:
        return STRUCTURE_IDS_FALLBACK.copy()


def fetch_json_items_from_structure(structure_id: int):
    """Busca itens JSON para um ID de estrutura espec√≠fico."""
    url = (
        f"{BASE_URL}/o/headless-delivery/v1.0/content-structures/{structure_id}/structured-contents"
        f"?pageSize=100&sort=datePublished:desc&filter=siteId eq 34276"
    )
    try:
        resp = SESSION.get(url, timeout=TIMEOUT)
        if resp.status_code == 200:
            data = resp.json()
            return data.get("items", [])
    except Exception:
        return []
    return []


def fetch_json_items_from_generic_sources():
    """Busca itens JSON de fontes gen√©ricas adicionais."""
    items = []
    for url in GENERIC_SOURCES:
        try:
            resp = SESSION.get(url, timeout=TIMEOUT)
            if resp.status_code == 200:
                data = resp.json()
                items.extend(data.get("items", []))
        except Exception:
            continue
    return items


def scrape_latest_from_html():
    """
    Raspagem real da p√°gina de not√≠cias principal.
    Objetivo: obter t√≠tulos, links e eventualmente imagem das not√≠cias exibidas na home/listagem.
    Como o HTML pode variar, usamos heur√≠sticas simples: anchors com hrefs para /w/noticia/ ou friendly URLs.
    """
    items = []
    try:
        resp = SESSION.get(NEWS_PAGE, timeout=TIMEOUT)
        if resp.status_code != 200 or not resp.text:
            return items

        html = resp.text

        # Heur√≠stica: capturar blocos de not√≠cia pelo padr√£o de links
        # Ex.: <a href="/w/noticia/...">T√≠tulo</a> ou <a href="https://prefeitura.sp.gov.br/w/noticia/...">
        # Tamb√©m capturar poss√≠veis imagens pr√≥ximas
        link_pattern = re.compile(r'href="([^"]+/w/noticia/[^"]+)"[^>]*>([^<]+)</a>', re.IGNORECASE)
        for href, anchor_text in link_pattern.findall(html):
            link = normalize_url(href.strip())
            title = anchor_text.strip()
            # Tenta capturar imagem pr√≥xima ao link (mesmo bloco)
            img_match = re.search(
                r'<img[^>]+src="([^"]+)"[^>]*',
                html
            )
            img_url = normalize_image_url(img_match.group(1)) if img_match else DEFAULT_IMAGE

            # Sem data no HTML: usa agora; JSON mais abaixo complementa
            items.append({
                "title": title,
                "contentUrl": link,
                "datePublished": datetime.now(timezone.utc).isoformat(),
                "contentFields": [{"name": "imagem", "contentFieldValue": {"image": {"contentUrl": img_url}}}]
            })

        return items
    except Exception:
        return []


def fetch_all_sources():
    """
    Estilo rss.app: descobre IDs din√¢micos na p√°gina, consulta m√∫ltiplas fontes JSON,
    complementa com raspagem HTML. Junta tudo, ordena e filtra.
    """
    combined = []

    # 1) Raspagem HTML da listagem principal (prioridade)
    html_items = scrape_latest_from_html()
    combined.extend(html_items)

    # 2) Descobrir dinamicamente IDs de estruturas atuais
    discovered_ids = discover_structure_ids_from_page()

    # 3) Buscar JSON para cada ID descoberto
    for sid in discovered_ids:
        sid_items = fetch_json_items_from_structure(sid)
        combined.extend(sid_items)

    # 4) Fontes gen√©ricas complementares
    combined.extend(fetch_json_items_from_generic_sources())

    # 5) Deduplica√ß√£o por link (contentUrl) mantendo o mais recente
    dedup = {}
    for it in combined:
        link = it.get("contentUrl") or ""
        if not link:
            # se n√£o tiver link, cria chave pela hash do t√≠tulo
            link = f"no-link-{hashlib.sha256(safe_title(it).encode()).hexdigest()}"
        dt = safe_date(it.get("datePublished"))
        if link not in dedup or dt > safe_date(dedup[link].get("datePublished")):
            dedup[link] = it

    items = list(dedup.values())

    # 6) Ordena por data decrescente
    items.sort(key=lambda x: safe_date(x.get("datePublished")), reverse=True)

    # 7) Filtra √∫ltimos 180 dias (ajust√°vel para seguran√ßa)
    cutoff = datetime.now(timezone.utc) - timedelta(days=180)
    items = [i for i in items if safe_date(i.get("datePublished")) >= cutoff]

    # 8) Garante no m√≠nimo MIN_ITEMS; se faltar, relaxa cutoff (pega mais antigos)
    if len(items) < MIN_ITEMS:
        # re-ordena toda base sem cutoff
        items = list(dedup.values())
        items.sort(key=lambda x: safe_date(x.get("datePublished")), reverse=True)

    # 9) Limita a 100 para n√£o pesar
    return items[:100]


# Constru√ß√£o do feed -------------------------------------------------------

def build_feed():
    """Constr√≥i o feed RSS com as √∫ltimas not√≠cias combinando raspagem e JSON."""
    fg = FeedGenerator()
    fg.title("Not√≠cias de Itaquera")
    fg.link(href=NEWS_PAGE)
    fg.description("Feed confi√°vel com as √∫ltimas not√≠cias da Prefeitura.")
    fg.language("pt-br")

    entries_added = 0
    news_items = fetch_all_sources()

    for item in news_items:
        title = safe_title(item)
        link = item.get("contentUrl") or NEWS_PAGE
        if not title or not link:
            continue

        dt = safe_date(item.get("datePublished"))

        # Conte√∫do e imagem
        content = ""
        img_url = None
        for field in item.get("contentFields", []):
            if not isinstance(field, dict):
                continue
            name = field.get("name", "").lower()
            if name in ["texto", "conteudo", "body"] and "contentFieldValue" in field:
                content = field["contentFieldValue"].get("data", "") or content
            if name in ["imagem", "image"] and "contentFieldValue" in field:
                raw_img = field["contentFieldValue"].get("image", {}).get("contentUrl")
                img_url = normalize_image_url(raw_img)

        if not img_url:
            img_url = DEFAULT_IMAGE

        # Aplica√ß√£o dos filtros
        full_text = f"{title} {content}"
        include_ok = True
        if INCLUDE_KEYWORDS:
            include_ok = any(k.lower() in full_text.lower() for k in INCLUDE_KEYWORDS)

        exclude_ok = True
        if EXCLUDE_KEYWORDS:
            exclude_ok = not any(k.lower() in full_text.lower() for k in EXCLUDE_KEYWORDS)

        if include_ok and exclude_ok:
            fe = fg.add_entry()
            fe.title(title)
            fe.link(href=link)
            fe.description(content if content else title)
            fe.enclosure(img_url, 0, "image/jpeg")
            fe.guid(hashlib.sha256(link.encode()).hexdigest(), permalink=False)
            fe.pubDate(dt)
            entries_added += 1

        # Para garantir no m√≠nimo 10 rapidamente, se j√° temos MIN_ITEMS, podemos parar (opcional)
        # if entries_added >= MIN_ITEMS:
        #     break

    # Se nada foi encontrado, adiciona item informativo
    if entries_added == 0:
        fe = fg.add_entry()
        fe.title("Sem not√≠cias no momento")
        fe.link(href=NEWS_PAGE)
        fe.description("Nenhum item foi encontrado com os filtros atuais.")
        fe.enclosure(DEFAULT_IMAGE, 0, "image/jpeg")
        fe.pubDate(datetime.now(timezone.utc))

    rss_bytes = fg.rss_str(pretty=True)
    return rss_bytes


# Endpoint com cache robusto ----------------------------------------------

@app.route("/feed.xml")
def feed():
    """
    Endpoint do feed RSS com cache:
    - Cache em mem√≥ria por 10 min
    - Cache persistente em disco usado como fallback
    - Nunca retorna vazio ao atualizar/F5 se j√° houve um feed v√°lido antes
    """
    now = time.time()

    # Serve cache em mem√≥ria se v√°lido
    if CACHE["feed"] and (now - CACHE["ts"] < CACHE_TTL):
        return Response(CACHE["feed"], mimetype="application/rss+xml")

    try:
        rss = build_feed()
        # Atualiza cache mem√≥ria e disco
        CACHE["feed"] = rss
        CACHE["ts"] = now
        write_disk_cache(rss)
        return Response(rss, mimetype="application/rss+xml")
    except Exception:
        # Fallback: serve √∫ltimo feed em mem√≥ria, ou disco, ou mensagem
        if CACHE["feed"]:
            return Response(CACHE["feed"], mimetype="application/rss+xml")
        disk = read_disk_cache()
        if disk:
            CACHE["feed"] = disk
            CACHE["ts"] = now
            return Response(disk, mimetype="application/rss+xml")
        return Response("Erro ao gerar feed", mimetype="text/plain")
